import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# Read the dataset from a file
file_path = "/content/preprocessed_dataset.csv"
df = pd.read_csv(file_path)

# Drop any remaining non-numeric columns
non_numeric_cols = df.select_dtypes(include=['object']).columns
df.drop(columns=non_numeric_cols, inplace=True)

# Splitting the dataset into features and target variable
X = df.drop(columns=['Type'])  # Features
y = df['Type']  # Target variable

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Step 2: Create a KNN regression model
model = KNeighborsRegressor(n_neighbors=5)

# Step 3: Fit the model to the training data
model.fit(X_train_scaled, y_train)

# Step 4: Evaluate the model's performance
# Predict on the training set
train_predictions = model.predict(X_train_scaled)
train_mse = mean_squared_error(y_train, train_predictions)
print("Training MSE:", train_mse)

# Predict on the testing set
test_predictions = model.predict(X_test_scaled)
test_mse = mean_squared_error(y_test, test_predictions)
print("Testing MSE:", test_mse)

# Visualize actual vs predicted values for the testing set
plt.figure(figsize=(10, 6))
plt.scatter(y_test, test_predictions, color='blue', alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Actual vs Predicted Values (Testing Set)')
plt.show()
