import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# Read the dataset from a file
file_path = "/content/preprocessed_dataset.csv"
df = pd.read_csv(file_path)

# Drop any remaining non-numeric columns
non_numeric_cols = df.select_dtypes(include=['object']).columns
df.drop(columns=non_numeric_cols, inplace=True)

# Splitting the dataset into features and target variable
X = df.drop(columns=['Type'])  # Features
y = df['Type']  # Target variable

# One-hot encode categorical variables
categorical_cols = df.select_dtypes(include=['object']).columns
for col in categorical_cols:
    X = pd.get_dummies(X, columns=[col], drop_first=True)

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 2: Create a Random Forest regressor model
model = RandomForestRegressor(random_state=42)

# Step 3: Fit the model to the training data
model.fit(X_train, y_train)

# Step 4: Evaluate the model's performance
# Predict on the training set
train_predictions = model.predict(X_train)
train_mse = mean_squared_error(y_train, train_predictions)
print("Training MSE:", train_mse)

# Predict on the testing set
test_predictions = model.predict(X_test)
test_mse = mean_squared_error(y_test, test_predictions)
print("Testing MSE:", test_mse)

import matplotlib.pyplot as plt

# Plotting the actual vs predicted values for the testing set
plt.figure(figsize=(10, 6))
plt.scatter(y_test, test_predictions, color='blue', alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Actual vs Predicted Values (Testing Set)')
plt.show()
